{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_Dongkeun_github.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg7UMlhNQn4e"
      },
      "source": [
        "# **실험개요**  \r\n",
        "1) Variational Auto Encoder 구현  \r\n",
        "2) 정상 데이터만을 이용하여 학습  \r\n",
        "3) 정상 데이터와 비정상 데이터에 대한 재생성 데이터간 유사도 측정  \r\n",
        "4) 유사도를 기반으로 비정상/정상 구분  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N64b4pOz2twD"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"autoencoders\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSL_4lUv2xcc"
      },
      "source": [
        "class Sampling(keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        mean, log_var = inputs\n",
        "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4y-UvcnD4-g"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "# 사용할 라이브러리 선언\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras import regularizers\n",
        "\n",
        "#for data preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#for modeling\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split #training and testing data split\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#filter warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "#data 로드\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/total_data/정상 심전도 신호 데이터.csv\")\n",
        "\n",
        "#data 분할\n",
        "X_train,X_test=train_test_split(data,test_size=0.2,random_state=RANDOM_SEED) #train_test_split을 이용하여 전체 데이터에서 train용과 test용을 분리\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_train=X_train.dropna(axis=1) # 데이터에서 NaN이 존재한다면 해당 열을 제거\n",
        "X_test = X_test.astype(float) / 255\n",
        "X_test=X_test.dropna(axis=1) # 데이터에서 NaN이 존재한다면 해당 열을 제거\n",
        "print(X_train.shape)\n",
        "input_dim = X_train.shape[1] # input 차원을 위하여 입력한 csv파일의 첫번째 shape값 사용\n",
        "print(input_dim)\n",
        "\n",
        "encoding_dim = input_dim\n",
        "\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "codings_size = 10\n",
        "\n",
        "K = keras.backend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11pmmmvW21_D"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "# 사용할 라이브러리 선언\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras import regularizers\n",
        "\n",
        "#for data preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#for modeling\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split #training and testing data split\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#filter warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "#data 로드\n",
        "#data_paf = pd.read_csv(\"/content/drive/My Drive/pafcut/merge_paf.csv\",header=None)\n",
        "data=pd.read_csv(\"/content/drive/My Drive/merge_real.csv\")\n",
        "\n",
        "#leave one out cross validation\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "\n",
        "\n",
        "#data 분할\n",
        "X_train,X_test=train_test_split(data,test_size=0.2,random_state=RANDOM_SEED) #train_test_split을 이용하여 전체 데이터에서 train용과 test용을 분리\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_train=X_train.dropna(axis=1) # 데이터에서 NaN이 존재한다면 해당 열을 제거\n",
        "X_test = X_test.astype(float) / 255\n",
        "X_test=X_test.dropna(axis=1) # 데이터에서 NaN이 존재한다면 해당 열을 제거\n",
        "print(X_train.shape)\n",
        "input_dim = X_train.shape[1] # input 차원을 위하여 입력한 csv파일의 첫번째 shape값 사용\n",
        "print(input_dim)\n",
        "\n",
        "\n",
        "encoding_dim = input_dim\n",
        "\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "codings_size = 10\n",
        "\n",
        "K = keras.backend\n",
        "z = keras.layers.Dense(encoding_dim, activation=\"selu\")(input_layer)\n",
        "z = keras.layers.Dense(encoding_dim/2, activation=\"selu\")(z)\n",
        "codings_mean = keras.layers.Dense(codings_size)(z)\n",
        "codings_log_var = keras.layers.Dense(codings_size)(z)\n",
        "codings = Sampling()([codings_mean, codings_log_var])\n",
        "variational_encoder = keras.models.Model(\n",
        "    inputs=[input_layer], outputs=[codings_mean, codings_log_var, codings])\n",
        "\n",
        "decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
        "x = keras.layers.Dense(encoding_dim/2, activation=\"selu\")(decoder_inputs)\n",
        "x = keras.layers.Dense(encoding_dim, activation=\"selu\")(x)\n",
        "variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[x])\n",
        "\n",
        "_, _, codings = variational_encoder(input_layer)\n",
        "reconstructions = variational_decoder(codings)\n",
        "variational_ae = keras.models.Model(inputs=[input_layer], outputs=[reconstructions])\n",
        "\n",
        "latent_loss = -0.5 * K.sum(\n",
        "    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n",
        "    axis=-1)\n",
        "variational_ae.add_loss(K.mean(latent_loss) / 784.)\n",
        "variational_ae.compile(loss=\"mse\", optimizer=\"Nadam\",  metrics=['accuracy'])\n",
        "checkpointer = ModelCheckpoint(filepath=\"./\",\n",
        "                               verbose=0,\n",
        "                               save_best_only=True)\n",
        "tensorboard = TensorBoard(log_dir='./logs',\n",
        "                          histogram_freq=0,\n",
        "                          write_graph=True,\n",
        "                          write_images=True)\n",
        "history = variational_ae.fit(X_train, X_train, epochs=200, batch_size=256,shuffle=True,\n",
        "                             validation_data=(X_test, X_test), #batch_size=256 to 512 수정 --> 오히려 성능이 떨어짐\n",
        "                             verbose=1,\n",
        "                    callbacks=[checkpointer, tensorboard]).history\n",
        "# 모델 저장\n",
        "model_json = variational_ae.to_json() \n",
        "with open(\"model_second.json\", \"w\") as json_file: \n",
        "  json_file.write(model_json)\n",
        "variational_ae.save_weights(\"model_second.h5\") \n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuhHAEJI244m"
      },
      "source": [
        "from tensorflow.compat.v2.keras.models import model_from_json\n",
        "\n",
        "\n",
        "json_file = open(\"/content/drive/MyDrive/model/model_vae.json\", \"r\") \n",
        "loaded_model_json = json_file.read() \n",
        "json_file.close()\n",
        "\n",
        "loaded_model = model_from_json(loaded_model_json,custom_objects={'Sampling': Sampling})\n",
        "\n",
        "loaded_model.load_weights(\"/content/drive/MyDrive/model/model_vae.h5\") \n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0jynmiv27Qm"
      },
      "source": [
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTd7z1kO28tC"
      },
      "source": [
        "predictions = loaded_model.predict(X_test)\n",
        "plt.plot(X_test.to_numpy()[3,:])\n",
        "plt.plot(predictions[3,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZYpLECM2-SP"
      },
      "source": [
        "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
        "error_df = pd.DataFrame({'reconstruction_error': mse})\n",
        "error_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f614UAne3Br2"
      },
      "source": [
        "import sklearn\n",
        "x_sk=sklearn.metrics.pairwise.cosine_similarity(X_test.to_numpy(), predictions, dense_output=True)\n",
        "print(x_sk.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlNzYGBtCmfB"
      },
      "source": [
        "sh=X_test.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il8W70eUEZ7w"
      },
      "source": [
        "print(sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7nKnpCuCcdu"
      },
      "source": [
        "cutoff = 0.69 #문턱값\n",
        "y_pred_classes_df2 = np.zeros(shape=(sh,)) # 0으로 채워진 넘파이 배열 생성\n",
        "#print(y_pred_classes.shape) #(37069,)로 잘 됨\n",
        "for k in range(sh):\n",
        "  x_sk=sklearn.metrics.pairwise.cosine_similarity(X_test.to_numpy()[k,:].reshape(1,121), predictions[k,:].reshape(1,121), dense_output=True)\n",
        "  if x_sk.mean()>cutoff: #문턱값보다 높다면 1로 채운다\n",
        "    y_pred_classes_df2[k]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFE3J9ppGYXG"
      },
      "source": [
        "from sklearn.model_selection import KFold # K-Fold Cross Validation \n",
        "from sklearn.model_selection import cross_val_score # 점수 평가\n",
        "from sklearn.model_selection import cross_val_predict # 예측\n",
        "from sklearn import metrics #accuracy measure\n",
        "from sklearn.metrics import confusion_matrix # confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqurVFUWGb9s"
      },
      "source": [
        "y_pred_classes_ori_df2=np.ones(shape=(sh,))\n",
        "sns.heatmap(confusion_matrix(y_pred_classes_ori_df2,y_pred_classes_df2), annot=True, fmt = '1.0f') #정상 heatmap 그리기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUni0fGD3EW8"
      },
      "source": [
        "# 비교를 위한 데이터 로드\n",
        "### 정상 데이터를 이용하여 학습하였으므로 여기서 로드하는 데이터는 비정상"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMPolTt93Qm7"
      },
      "source": [
        "data2=pd.read_csv(\"/content/drive/MyDrive/total_data/비정상 심전도 데이터.csv\")\n",
        "data2=data2.dropna(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd8GB5jT3TCF"
      },
      "source": [
        "X_train2,X_test2=train_test_split(data2,test_size=0.2,random_state=RANDOM_SEED) #train_test_split을 이용하여 전체 데이터에서 train용과 test용을 분리\n",
        "X_train2 = X_train2.astype(float) / 255\n",
        "#print(X_train2) # 데이터에 마지막 열을 읽어오는데 NaN이 존재한다\n",
        "X_train2=X_train2.dropna(axis=1) # 데이터에서 NaN이 존재한다면 해당 열을 제거\n",
        "#print(X_train2)\n",
        "tmp=X_train2.to_numpy()\n",
        "plt.plot(tmp[0,:]) # ecg 데이터 하나만 그래프로 도식화 해보자\n",
        "X_test2 = X_test2.astype(float) / 255\n",
        "X_test2=X_test2.dropna(axis=1) # 데이터에서 NaN이 존재한다면 해당 열을 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccKEc4Q53UcA"
      },
      "source": [
        "plt.plot(tmp[100,:]) # ecg 데이터 하나만 그래프로 도식화 해보자\n",
        "predictions2 = loaded_model.predict(X_test2)\n",
        "plt.plot(predictions2[100,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiSyUYOv3Vt8"
      },
      "source": [
        "mse2 = np.mean(np.power(X_test2 - predictions2, 2), axis=1)\n",
        "error_df = pd.DataFrame({'reconstruction_error': mse2})\n",
        "error_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvFAmEgB3Ypk"
      },
      "source": [
        "import sklearn\n",
        "x_sk2=sklearn.metrics.pairwise.cosine_similarity(X_test2.to_numpy(), predictions2, dense_output=True)\n",
        "print(x_sk2)\n",
        "print(x_sk2.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SFNNNs5CDOS"
      },
      "source": [
        "cutoff2 = 0.69 #문턱값\n",
        "y_pred_classes_df22 = np.ones(shape=(sh,)) # 0으로 채워진 넘파이 배열 생성\n",
        "#print(y_pred_classes.shape) #(37069,)로 잘 됨\n",
        "for k in range(sh):\n",
        "  x_sk2=sklearn.metrics.pairwise.cosine_similarity(X_test2.to_numpy()[k,:].reshape(1,121), predictions2[k,:].reshape(1,121), dense_output=True)\n",
        "  if x_sk2.mean()<cutoff2: #문턱값보다 높다면 1로 채운다\n",
        "    y_pred_classes_df22[k]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQgfoVJlGzez"
      },
      "source": [
        "y_pred_classes_ori_df22=np.zeros(shape=(sh,))\n",
        "sns.heatmap(confusion_matrix(y_pred_classes_ori_df22,y_pred_classes_df22), annot=True, fmt = '1.0f') #정상 heatmap 그리기"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}